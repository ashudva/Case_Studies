extract(source, "source", regex = "Twitter for (.*)")
extract(source, "source", regex = "Twitter for (.*)") $.source
extract(source, "source", regex = "Twitter for (.*)") %>% .$source
trump_tweets %>%
extract(source, "source", regex = "Twitter for (.*)") %>% .$source
str(trump_tweets)
trump_tweets %>%
extract(source, "source", regex = "Twitter for (.*)") %>% .$source %>% count
extract(source, "source", regex = "Twitter for (.*)")  %>% count(sounrce)
extract(source, "source", regex = "Twitter for (.*)")  %>% count(source)
trump_tweets %>%
extract(source, "source", regex = "Twitter for (.*)")  %>% count(source)
campaign_tweets <- trump_tweets %>%
extract(source, "source", regex = "Twitter for (.*)")  %>%
filter(created_at >= ymd("2015-06-17") & created_at < ymd("2016-11-08") & source %in% c("Android", "iPhone")) %>%
filter(!is_retweet) %>%
arrange(created_at)
campaign_tweets
campaign_tweets %>% as_tibble()
campaign_tweets <-  campaign_tweets %>% as_tibble()
campaign_tweets
str(campaign_tweets)
?n
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>% head
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour)
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source)
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source) %>% mutate(percent = n / sum(n))
group_by(source) %>% mutate(percent = n(source) / sum(n(source)))
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source) %>% mutate(percent = n(source) / sum(n(source)))
group_by(source) %>% mutate(percent = n() / sum(n()))
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source) %>% mutate(percent = n() / sum(n()))
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source) %>% mutate(percent = n() / sum(n())) %>% max(source)
group_by(source) %>% mutate(percent = n() / sum(n())) %>% max(.$source)
group_by(source) %>% mutate(percent = n() / sum(n())) %>% .$percent %>% max()
group_by(source) %>% mutate(percent = n() / sum(n())) %>% .$percent
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source) %>% mutate(percent = n() / sum(n())) %>% .$percent %>% max
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>% group_by(source) %>%
mutate(percent = n / sum(n)) %>% ungroup %>% ggplot() +
geom_line(aes(hour, percent, color = source)) +
geom_point(aes(hour, percent, color = source))
ds_theme_set()
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>%
group_by(source) %>%
mutate(percent = n / sum(n)) %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
geom_point() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)", y = "% of tweets", color = "")
ds_theme_set()
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>%
group_by(source) %>%
mutate(percent = n / sum(n)) %>%
ungroup %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
geom_point() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)", y = "% of tweets", color = "")
ds_theme_set()
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>%
group_by(source) %>%
mutate(percent = n / sum(n)) %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
geom_point() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)", y = "% of tweets", color = "")
?ungroup
install.packages("tidytext")
library(tidytext)
example <- data_frame(line = c(1, 2, 3, 4),
text = c("Roses are red,", "Violets are blue,", "Sugar is sweet,", "And so are you."))
example
example %>% unnest_tokens(word, text)
example <- tibble(line = c(1, 2, 3, 4),
text = c("Roses are red,", "Violets are blue,", "Sugar is sweet,", "And so are you."))
example
example %>% unnest_tokens(word, text)
q("yes")
library(tidytext)
library(tidyverse)
library(lubridate)
library(dslabs)
data("trump_tweets")
head(trump_tweets)
rm(trump_tweets)
poem <- c("This is fucking aweosome", "Life is a lie", "God is a lie", "Childhood mind is its own worst enemy", "Schools are for fools or ediots or fatuous people")
tibble(poem)
tibble(line = c(1, 2, 3, 4, 5), text = poem)
tibble(line = 1:5, text = poem)
example <- tibble(line = 1:5, text = poem)
example
example %>% unnest_tokens()
example %>% unnest_tokens(words, text)
View(example %>% unnest_tokens(words, text))
?unnest_tokens
View(example %>% unnest_ngrams(words, text))
str(campaign_tweets)
i <- 3008
campaign_tweets[i,]
i <- 30008
campaign_tweets[i,]
i <- 3008
campaign_tweets[i,]
campaign_tweets[i,] %>%
unnest_tokens(words, text) %>%
.$words
campaign_tweets[i,] %>%
unnest_tokens(words, text) %>%
pull(words)
campaign_tweets$text[i] %>% str_wrap(width = 65) %>% cat
campaign_tweets[i,] %>%
unnest_tokens(words, text) %>%
campaign_tweets[i,] %>%
unnest_tokens(words, text, token = "tweets") %>%
pull(words)
?mutate_at
links <- "https://t.co/[a-zA-Z"
links <- "https://t.co/[a-zA-Z]+|&amp"
tweet_words <- campaign_tweets %>% mutate(text = str_replace_all(text, links, "")) %>%
unnest_tokens(words, text, tokens = "tweets") %>% pull(words)
unnest_tokens(words, text, token = "tweets") %>% pull(words)
tweet_words <- campaign_tweets %>% mutate(text = str_replace_all(text, links, "")) %>%
unnest_tokens(words, text, token = "tweets") %>% pull(words)
head(tweet_words)
View(tweet_words)
tweet_words %>% count(words) %>% arrange(desc(n))
tweet_words <- campaign_tweets %>% mutate(text = str_replace_all(text, links, "")) %>%
unnest_tokens(words, text, token = "tweets")
tweet_words %>% count(words) %>% arrange(desc(n))
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, links, ""))  %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word )
tweet_words %>% count(words) %>% arrange(desc(n))
tweet_words
tweet_words %>% count(word) %>% arrange(desc(n))
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, links, ""))  %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
?count
library(dslabs)
library(tidyverse)
library(tidytext)
install.packages("textdata")
library(textdata)
get_sentiments("bing")
head(campaign_tweets)
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, links, ""))  %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
links <- "https://t.co/[A-Za-z\\d]+|&amp;"
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, links, ""))  %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
android_iphone_or <- tweet_words %>%
count(word, source) %>%
spread(source, n, fill = 0) %>%
mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) /
( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
get_sentiments("afinn")
get_sentiments("nrc")
tweet_words %>% inner_join(get_sentiments$sentiment, by = "word") %>% top_n(n = 10)
tweet_words %>% inner_join(get_sentiments, by = "word") %>% head()
head(tweet_words)
tweet_words %>% inner_join(get_sentiments, by = "word")
rlang::last_error()
tweet_words
tweet_words %>% inner_join(get_sentiments, by = "word", copy = )
tweet_words %>% inner_join(get_sentiments, by = "word", copy = F)
nrc <- get_sentiments("nrc") %>% select(word, sentiments)
nrc <- get_sentiments("nrc") %>% select(word, sentiment)
tweet_words %>% inner_join(nrc, by = "word", copy = F)
tweet_words %>% inner_join(nrc, by = "word", copy = F) %>% select(source, word, sentiment) %>% count(source, sentiment)
tweet_words %>% inner_join(nrc, by = "word", copy = F) %>% select(source, word, sentiment) %>% count(source, sentiment) %>%
spread(sentiment, n)
sentiment_count <- tweet_words %>% inner_join(nrc, by = "word", copy = F) %>% select(source, word, sentiment) %>% count(source, sentiment) %>%
spread(sentiment, n)
sentiment_count
sentiment_count %>% ggplot(aes(color = sentiment)) %>%
sentiment_count %>% ggplot(aes(color = sentiment)) + geom_histogram()
sentiment_counts <- tweet_words %>%
left_join(nrc, by = "word") %>%
count(source, sentiment) %>%
spread(source, n) %>%
mutate(sentiment = replace_na(sentiment, replace = "none"))
sentiment_counts
tweet_words %>% group_by(source) %>% summarize(n = n())
android_iphone_or %>% inner_join(nrc, by = "word") %>%
mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
mutate(log_or = log(or)) %>%
filter(Android + iPhone > 10 & abs(log_or)>1) %>%
mutate(word = reorder(word, log_or)) %>%
ggplot(aes(word, log_or, fill = log_or < 0)) +
facet_wrap(~sentiment, scales = "free_x", nrow = 2) +
geom_bar(stat="identity", show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
install.packages("broom")
library(broom)
log_or <- sentiment_counts %>%
mutate( log_or = log( (Android / (sum(Android) - Android)) / (iPhone / (sum(iPhone) - iPhone))),
se = sqrt( 1/Android + 1/(sum(Android) - Android) + 1/iPhone + 1/(sum(iPhone) - iPhone)),
conf.low = log_or - qnorm(0.975)*se,
conf.high = log_or + qnorm(0.975)*se) %>%
arrange(desc(log_or))
log_or
library(tidyverse)
log_or <- sentiment_counts %>%
mutate( log_or = log( (Android / (sum(Android) - Android)) / (iPhone / (sum(iPhone) - iPhone))),
se = sqrt( 1/Android + 1/(sum(Android) - Android) + 1/iPhone + 1/(sum(iPhone) - iPhone)),
conf.low = log_or - qnorm(0.975)*se,
conf.high = log_or + qnorm(0.975)*se) %>%
arrange(desc(log_or))
log_or
log_or %>%
mutate(sentiment = reorder(sentiment, log_or),) %>%
ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
geom_errorbar() +
geom_point(aes(sentiment, log_or)) +
ylab("Log odds ratio for association between Android and sentiment") +
coord_flip()
android_iphone_or %>% inner_join(nrc, by = "word") %>%
mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
mutate(log_or = log(or)) %>%
filter(Android + iPhone > 10 & abs(log_or)>1) %>%
mutate(word = reorder(word, log_or)) %>%
ggplot(aes(word, log_or, fill = log_or < 0)) +
facet_wrap(~sentiment, scales = "free_x", nrow = 2) +
geom_bar(stat="identity", show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
data(brexit_polls)
library(dslabs)
data("brexit_polls")
str(brexit_polls)
count(brexit_polls$startdate)
length(brexit_polls$startdate)
library(tidyverse)
library(dslabs)
library(broom)
options(digits = )
options(digits = 3)
data("brexit_polls")
month(brexit_polls$startdate)
library(lubridate)
month(brexit_polls$startdate)
sum(month(brexit_polls$startdate) == 4)
?round_date
str(brexit_polls)
head(brexit_polls$enddate)
round_date(brexe)
round_date(brexit_polls$enddate, unit = "week")
sum(round_date(brexit_polls$enddate, unit = "week") == "2016-06-12")
weekdays(brexit_polls$enddate)
count(weekdays(brexit_polls$enddate))
brexit_polls <- brexit_polls %>% mutate(endday = weekdays(enddate))
brexit_polls
group_by(brexit_polls, endday)
group_by(brexit_polls, endday) %>% count(endday)
group_by(brexit_polls, endday) %>% count(endday) %>% max(n)
group_by(brexit_polls, endday) %>% count(endday)
data("movielens")
str(movielens)
movielens %>% tibble()
movielens %>% tibble() %>% count(genres)
movielens %>% tibble() %>% count(genres, movies)
?as_datetime
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01"))
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_year = year(timestamp)) %>% group_by(r_year) %>% count(r_year)
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_year = year(timestamp)) %>% group_by(r_year) %>% count(r_year) %>% .$n
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_year = year(timestamp)) %>% group_by(r_year) %>% count(r_year) %>% .$n %>% maz
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_year = year(timestamp)) %>% group_by(r_year) %>% count(r_year) %>% .$n %>% max()
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_hour = hour(timestamp)) %>% group_by(r_year) %>% count(r_year) %>% .$n %>% max()
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_hour = hour(timestamp)) %>% group_by(r_hour) %>% count(r_hour) %>% .$n %>% max()
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_hour = hour(timestamp)) %>% group_by(r_hour) %>% count(r_hour) %>% .$n
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_hour = hour(timestamp)) %>% group_by(r_hour) %>% count(r_hour)
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_hour = hour(timestamp)) %>% group_by(r_hour) %>% count(r_hour) %>% View()
setwd('Case_Studies/Hurricane/')
FN
fn
load("C:/Users/ashish/Work/Case_Studies/Hurricane/.RData")
fn
library(pdftools)
txt <- pdf_text(fn)
txt
str(txt)
class(txt)
txt[9]
library(tidyverse)
x <- str_split(txt[9], "\\n")
x
x[1]
x[,1]
x[1,]
x[[1],]
x[1][1]
x[1]
cat(x)
sapply(x, cat)
class(x)
length(x)
x <- str_split(txt[9], "\n")
x
s <- x[1]
class()
class(s)
s
s[]
s[1]
x
x[[1]]
s <- x[[1]]
class(s)
lenght(s)
length(s)
s <- str_trim(s)
s[[1]]
s[[1]][-1]
s[[1]]
s[[-1]]
s[[1]][40]
s[[1]][length(s)]
s[[1]][length(s)-1]
s[[1][40]]
s[[1,40]]
s
header_index <- str_which
header_index <- str_which(s, "2015")
header_index
s[header_index]
s[2 25]
s[2 24]
s[2]
s[24]
header_index <- str_which(s, "2015")[1]
s[header_index]
header <- s[header_index]
head()
header
str_split(header, "\\s*")
str_split(header, "\\s+")
str_split(header, "\\s")
str_split(header, "\\s+")
month <- str_split(header, "\\s+")[1]
month
class(month)
month[[1]]
as.character(month)
as.character(month)[1]
as.character(month)[[1]]
month <- str_split(header, "\\s+", simplify = T)[1]
month
header
header <- str_split(s[header_index], "\\s+", simplify = T)[2:]
header <- str_split(s[header_index], "\\s+", simplify = T)
header
header <- str_split(s[header_index], "\\s+", simplify = T)[,2:]
header <- str_split(s[header_index], "\\s+", simplify = T)[1,2:]
class(header)
header[1,2]
header[1,2:5]
header[1,2:]
header <- header[1,2:]
header <- header[1,2:5]
header
header[3]
s
tail_index <- str_which(s, "Total")
tail_index
s[tail_index]
n <- str_count(s, pattern = "\\d+")
n
length(s[-1:-header_index, -tail_index:-length(s), -which(n == 1)])
s <- s[-1:-header_index]
s
s <- s[-tail_index:length(s)]
s <- s[-tail_index:-length(s)]
s <- s[-which(n == 1)]
s
s <- s[-length(s)]
s
s <- s[-length(s)]
s
str_remove_all(s, "[^\\d\\s]")
s
s <- str_remove_all(s, "[^\\d\\s]")
s
s <- str_split_fixed(s, "\\s+", n = 6)[,1:5]
s
names(s)
names(s) <- c("day", header)
s
s <- str_split_fixed(s, "\\s+", n = 6)[,1:5]
s
s <- x[[1]]
s
ind <- c(tail_index:length(s), 1:header_index, which(n == 1))
s <- s[-ind]
length(s)
s
s <- str_split_fixed(s, "\\s+", n = 6)[,1:5]
s
s <- s[-ind]
s <- str_trim()
s <- str_trim(s)
s
s <- x[[1]]
s <- str_trim(s)
s <- s[-ind]
s <- str_split_fixed(s, "\\s+", n = 6)[,1:5]
s
tmp <- s
colnames(s)
colnames(s) <- c("day", header)
colnames(s)
s
as.numeric(s)
apply(s, 1, as.numeric)
apply(s, 2, as.numeric)
s <- apply(s, 2, as.numeric)
s
?rep
?cbind
rep(month, length(s))
rep(month, lenght(s[,1]))
rep(month, lenght(s[day]))
rep(month, lenght(s["day"]))
rep(month, length(s["day"]))
rep(month, length(s[,1]))
length(rep(month, length(s[,1])))
col <- rep(month, length(s[,1]))
tmp
temp <- s
colbind(s, col)
cbind(s, col)
s
cbind(s, col)
cbind(s, col) %>% apply(2, as.numeric)
cbind(s,month = col)
tibble(s)
tibble(s) %>% as.matrix()
tibble(s) %>% mutate(month = col)
tibble(s) %>% mutate(month = col) %>% as.matrix()
s
mean(s[,2])
options(digits = 3)
mean(s[,2])
mean(s[,2:5])
mean(s[,3])
s[1:19, 4]
mean(s[1:19, 4])
mean(s[-1:-19, 4])
tab <- s %>%
as_data_frame() %>%
setNames(c("day", header)) %>%
mutate_all(as.numeric)
mean(tab$"2015")
tab <- s %>%
as_tibble() %>%
setNames(c("day", header)) %>%
mutate_all(as.numeric)
mean(tab$"2015")
tab
tab <- tab %>% gather(year, deaths, -day) %>%
mutate(deaths = as.numeric(deaths))
tab
tab %>% ggplot(aes(day, deaths, color = year)) + geom_line()
tab %>% ggplot(aes(day, deaths, color = year)) + geom_point() + geom_line()
tab %>% ggplot(aes(day, deaths, color = year)) + geom_point() + geom_line() +geom_vline(xintercept = 20)
tab %>% ggplot(aes(day, deaths, color = year)) + geom_point() + geom_line() +geom_vline(xintercept = 20, type = "dashed")
tab %>% ggplot(aes(day, deaths, color = year)) + geom_point() + geom_line() +geom_vline(xintercept = 20, linetype = "dashed")
tab[-"2018"]
tab[-2018]
s
s[-5]
s[,-5]
tab <- s[, -5] %>%
as_tibble() %>%
setNames(c("day", header)) %>%
mutate_all(as.numeric)
mean(tab$"2015")
tab %>% ggplot(aes(day, deaths, color = year)) + geom_point() + geom_line() +geom_vline(xintercept = 20, linetype = "dashed")
tab <- tab %>% gather(year, deaths, -day) %>%
mutate(deaths = as.numeric(deaths))
tab
tab %>% ggplot(aes(day, deaths, color = year)) + geom_point() + geom_line() +geom_vline(xintercept = 20, linetype = "dashed")
tab %>% ggplot(aes(day, deaths, color = year, size = 3)) + geom_point() + geom_line() +geom_vline(xintercept = 20, linetype = "dashed")
tab %>% ggplot(aes(day, deaths, color = year, size = 2)) + geom_point() + geom_line() +geom_vline(xintercept = 20, linetype = "dashed")
tab %>% ggplot(aes(day, deaths, color = year)) + geom_point() + geom_line() +geom_vline(xintercept = 20, linetype = "dashed")
