TRUE ~ as.numeric(NA))) %>%
select(-guess)
head(new_heights)
new_heights
library(dslabs)
data("research_funding_rates")
research_funding_rates
install.packages("pdftools")
library(pdftools)
tempfile <- tempfile()
url <- "http://www.pnas.org/content/suppl/2015/09/16/1510159112.DCSupplemental/pnas.201510159SI.pdf"
file.remove(tempfile)
list.files()
getwd()
temp_file <- tempfile
temp_file <- tempfile()
temp_file
download.file(url, temp_file)
text <- pdf_text(temp_file)
text()
text
ls()
txt <- pdf_text(temp_file)
txt
library("pdftools")
temp_file <- tempfile()
url <- "http://www.pnas.org/content/suppl/2015/09/16/1510159112.DCSupplemental/pnas.201510159SI.pdf"
download.file(url, temp_file)
txt <- pdf_text(temp_file)
file.remove(temp_file)
txt
raw_data_research_funding_rates <- txt[2]
raw_data_research_funding_rates
raw_data_research_funding_rates %>% head
tab <- str_split(raw_data_research_funding_rates, "\n")
tab
tab[[1]]
library(rvest)
library(tidyverse)
library(stringr)
url <- "https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_United_Kingdom_European_Union_membership_referendum&oldid=896735054"
tab <- read_html(url) %>% html_nodes("table")
polls <- tab[[5]] %>% html_table(fill = TRUE)
tab %>% head()
polls
str(polls)
names(polls)
names(polls) <- c("dates", "remain", "leave", "undecided", "lead", "samplesize", "pollster", "poll_type", "notes")
names(polls)
head(polls$remain)
polls[str_detect(polls$remain, "%")]
rlang::last_error()
polls$remain
str_detect(polls$remain, "%")
ind <- str_detect(polls$remain, "%")
polls[ind]
str(polls)
polls$remain[ind]
sum(ind)
polls %>% str_detect(.$remain, "%")
polls %>% str_detect(remain, "%")
polls %>% str_detect(.remain, "%")
polls %>% str_detect($remain, "%")
polls[1]
polls[,ind]
polls[ind,]
polls[,3]
polls[,2]
library(rvest)
library(tidyverse)
library(stringr)
url <- "https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_United_Kingdom_European_Union_membership_referendum&oldid=896735054"
tab <- read_html(url) %>% html_nodes("table")
polls <- tab[[5]] %>% html_table(fill = TRUE)
head(polld)
head(polls)
rm(list = ls())
library(rvest)
library(tidyverse)
library(stringr)
url <- "https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_United_Kingdom_European_Union_membership_referendum&oldid=896735054"
tab <- read_html(url) %>% html_nodes("table")
polls <- tab[[5]] %>% html_table(fill = TRUE)
names(polls) <- c("dates", "remain", "leave", "undecided", "lead", "samplesize", "pollster", "poll_type", "notes")
names(polls)
str_detect(polls$remain, "%")
ind <- str_detect(polls$remain, "%")
sum(ind)
remain_per <- polls$remain[ind]
as.numeric(str_remove(remain_per, "%"))
as.numeric(str_remove(remain_per, "%")) /100
parse_number(remain_per)
str_remove(remain_per, "%") /100
as.numeric(str_replace(polls$remain, "%", ""))/100
polls
head(polls$und\)
head(polls$undecided)
polls$undecided
str_replace(polls$undecided, "NA", "0%")
str_replace(polls$undecided, "NA|N/A", "0%")
class(NA)
str_replace(polls$undecided, NA, "0%")
pattern <- ""
pattern <- "([\\d+])\\s+([a-zA-Z]{3,4,5})"
temp <- str_extract_all(polls$dates, pattern)
end_date <- sapply(temp, function(x) x[length(x)])
install.packages("lubridates")
library(lubridate)
options(digits = 3)
library(tidyverse)
library(lubridate)
library(scales)
?discard
library(dslabs)
data("trump_tweets")
head(trump_tweets)
str(trump_tweets)
trump_tweets$text[16413]
trump_tweets$text[16413] %>% str_wrap(width = options()$width) %>% cat
trump_tweets$text[1643] %>% str_wrap(width = options()$width) %>% cat
trump_tweets$text[16453] %>% str_wrap(width = options()$width) %>% cat
trump_tweets$text[16453] %>% cat
?trump_tweets
trump_tweets %>% count(source)
trump_tweets %>% count(source) %>% arrange(desc(n))
trump_tweets %>%
extract(source, "source", regex = "Twitter for (.*)")
extract(source, "source", regex = "Twitter for (.*)") $.source
extract(source, "source", regex = "Twitter for (.*)") %>% .$source
trump_tweets %>%
extract(source, "source", regex = "Twitter for (.*)") %>% .$source
str(trump_tweets)
trump_tweets %>%
extract(source, "source", regex = "Twitter for (.*)") %>% .$source %>% count
extract(source, "source", regex = "Twitter for (.*)")  %>% count(sounrce)
extract(source, "source", regex = "Twitter for (.*)")  %>% count(source)
trump_tweets %>%
extract(source, "source", regex = "Twitter for (.*)")  %>% count(source)
campaign_tweets <- trump_tweets %>%
extract(source, "source", regex = "Twitter for (.*)")  %>%
filter(created_at >= ymd("2015-06-17") & created_at < ymd("2016-11-08") & source %in% c("Android", "iPhone")) %>%
filter(!is_retweet) %>%
arrange(created_at)
campaign_tweets
campaign_tweets %>% as_tibble()
campaign_tweets <-  campaign_tweets %>% as_tibble()
campaign_tweets
str(campaign_tweets)
?n
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>% head
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour)
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source)
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source) %>% mutate(percent = n / sum(n))
group_by(source) %>% mutate(percent = n(source) / sum(n(source)))
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source) %>% mutate(percent = n(source) / sum(n(source)))
group_by(source) %>% mutate(percent = n() / sum(n()))
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source) %>% mutate(percent = n() / sum(n()))
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source) %>% mutate(percent = n() / sum(n())) %>% max(source)
group_by(source) %>% mutate(percent = n() / sum(n())) %>% max(.$source)
group_by(source) %>% mutate(percent = n() / sum(n())) %>% .$percent %>% max()
group_by(source) %>% mutate(percent = n() / sum(n())) %>% .$percent
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
group_by(source) %>% mutate(percent = n() / sum(n())) %>% .$percent %>% max
campaign_tweets %>% mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>% group_by(source) %>%
mutate(percent = n / sum(n)) %>% ungroup %>% ggplot() +
geom_line(aes(hour, percent, color = source)) +
geom_point(aes(hour, percent, color = source))
ds_theme_set()
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>%
group_by(source) %>%
mutate(percent = n / sum(n)) %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
geom_point() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)", y = "% of tweets", color = "")
ds_theme_set()
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>%
group_by(source) %>%
mutate(percent = n / sum(n)) %>%
ungroup %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
geom_point() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)", y = "% of tweets", color = "")
ds_theme_set()
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>%
group_by(source) %>%
mutate(percent = n / sum(n)) %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
geom_point() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)", y = "% of tweets", color = "")
?ungroup
install.packages("tidytext")
library(tidytext)
example <- data_frame(line = c(1, 2, 3, 4),
text = c("Roses are red,", "Violets are blue,", "Sugar is sweet,", "And so are you."))
example
example %>% unnest_tokens(word, text)
example <- tibble(line = c(1, 2, 3, 4),
text = c("Roses are red,", "Violets are blue,", "Sugar is sweet,", "And so are you."))
example
example %>% unnest_tokens(word, text)
q("yes")
library(tidytext)
library(tidyverse)
library(lubridate)
library(dslabs)
data("trump_tweets")
head(trump_tweets)
rm(trump_tweets)
poem <- c("This is fucking aweosome", "Life is a lie", "God is a lie", "Childhood mind is its own worst enemy", "Schools are for fools or ediots or fatuous people")
tibble(poem)
tibble(line = c(1, 2, 3, 4, 5), text = poem)
tibble(line = 1:5, text = poem)
example <- tibble(line = 1:5, text = poem)
example
example %>% unnest_tokens()
example %>% unnest_tokens(words, text)
View(example %>% unnest_tokens(words, text))
?unnest_tokens
View(example %>% unnest_ngrams(words, text))
str(campaign_tweets)
i <- 3008
campaign_tweets[i,]
i <- 30008
campaign_tweets[i,]
i <- 3008
campaign_tweets[i,]
campaign_tweets[i,] %>%
unnest_tokens(words, text) %>%
.$words
campaign_tweets[i,] %>%
unnest_tokens(words, text) %>%
pull(words)
campaign_tweets$text[i] %>% str_wrap(width = 65) %>% cat
campaign_tweets[i,] %>%
unnest_tokens(words, text) %>%
campaign_tweets[i,] %>%
unnest_tokens(words, text, token = "tweets") %>%
pull(words)
?mutate_at
links <- "https://t.co/[a-zA-Z"
links <- "https://t.co/[a-zA-Z]+|&amp"
tweet_words <- campaign_tweets %>% mutate(text = str_replace_all(text, links, "")) %>%
unnest_tokens(words, text, tokens = "tweets") %>% pull(words)
unnest_tokens(words, text, token = "tweets") %>% pull(words)
tweet_words <- campaign_tweets %>% mutate(text = str_replace_all(text, links, "")) %>%
unnest_tokens(words, text, token = "tweets") %>% pull(words)
head(tweet_words)
View(tweet_words)
tweet_words %>% count(words) %>% arrange(desc(n))
tweet_words <- campaign_tweets %>% mutate(text = str_replace_all(text, links, "")) %>%
unnest_tokens(words, text, token = "tweets")
tweet_words %>% count(words) %>% arrange(desc(n))
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, links, ""))  %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word )
tweet_words %>% count(words) %>% arrange(desc(n))
tweet_words
tweet_words %>% count(word) %>% arrange(desc(n))
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, links, ""))  %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
?count
library(dslabs)
library(tidyverse)
library(tidytext)
install.packages("textdata")
library(textdata)
get_sentiments("bing")
head(campaign_tweets)
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, links, ""))  %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
links <- "https://t.co/[A-Za-z\\d]+|&amp;"
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, links, ""))  %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
android_iphone_or <- tweet_words %>%
count(word, source) %>%
spread(source, n, fill = 0) %>%
mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) /
( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
get_sentiments("afinn")
get_sentiments("nrc")
tweet_words %>% inner_join(get_sentiments$sentiment, by = "word") %>% top_n(n = 10)
tweet_words %>% inner_join(get_sentiments, by = "word") %>% head()
head(tweet_words)
tweet_words %>% inner_join(get_sentiments, by = "word")
rlang::last_error()
tweet_words
tweet_words %>% inner_join(get_sentiments, by = "word", copy = )
tweet_words %>% inner_join(get_sentiments, by = "word", copy = F)
nrc <- get_sentiments("nrc") %>% select(word, sentiments)
nrc <- get_sentiments("nrc") %>% select(word, sentiment)
tweet_words %>% inner_join(nrc, by = "word", copy = F)
tweet_words %>% inner_join(nrc, by = "word", copy = F) %>% select(source, word, sentiment) %>% count(source, sentiment)
tweet_words %>% inner_join(nrc, by = "word", copy = F) %>% select(source, word, sentiment) %>% count(source, sentiment) %>%
spread(sentiment, n)
sentiment_count <- tweet_words %>% inner_join(nrc, by = "word", copy = F) %>% select(source, word, sentiment) %>% count(source, sentiment) %>%
spread(sentiment, n)
sentiment_count
sentiment_count %>% ggplot(aes(color = sentiment)) %>%
sentiment_count %>% ggplot(aes(color = sentiment)) + geom_histogram()
sentiment_counts <- tweet_words %>%
left_join(nrc, by = "word") %>%
count(source, sentiment) %>%
spread(source, n) %>%
mutate(sentiment = replace_na(sentiment, replace = "none"))
sentiment_counts
tweet_words %>% group_by(source) %>% summarize(n = n())
android_iphone_or %>% inner_join(nrc, by = "word") %>%
mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
mutate(log_or = log(or)) %>%
filter(Android + iPhone > 10 & abs(log_or)>1) %>%
mutate(word = reorder(word, log_or)) %>%
ggplot(aes(word, log_or, fill = log_or < 0)) +
facet_wrap(~sentiment, scales = "free_x", nrow = 2) +
geom_bar(stat="identity", show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
install.packages("broom")
library(broom)
log_or <- sentiment_counts %>%
mutate( log_or = log( (Android / (sum(Android) - Android)) / (iPhone / (sum(iPhone) - iPhone))),
se = sqrt( 1/Android + 1/(sum(Android) - Android) + 1/iPhone + 1/(sum(iPhone) - iPhone)),
conf.low = log_or - qnorm(0.975)*se,
conf.high = log_or + qnorm(0.975)*se) %>%
arrange(desc(log_or))
log_or
library(tidyverse)
log_or <- sentiment_counts %>%
mutate( log_or = log( (Android / (sum(Android) - Android)) / (iPhone / (sum(iPhone) - iPhone))),
se = sqrt( 1/Android + 1/(sum(Android) - Android) + 1/iPhone + 1/(sum(iPhone) - iPhone)),
conf.low = log_or - qnorm(0.975)*se,
conf.high = log_or + qnorm(0.975)*se) %>%
arrange(desc(log_or))
log_or
log_or %>%
mutate(sentiment = reorder(sentiment, log_or),) %>%
ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
geom_errorbar() +
geom_point(aes(sentiment, log_or)) +
ylab("Log odds ratio for association between Android and sentiment") +
coord_flip()
android_iphone_or %>% inner_join(nrc, by = "word") %>%
mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
mutate(log_or = log(or)) %>%
filter(Android + iPhone > 10 & abs(log_or)>1) %>%
mutate(word = reorder(word, log_or)) %>%
ggplot(aes(word, log_or, fill = log_or < 0)) +
facet_wrap(~sentiment, scales = "free_x", nrow = 2) +
geom_bar(stat="identity", show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
data(brexit_polls)
library(dslabs)
data("brexit_polls")
str(brexit_polls)
count(brexit_polls$startdate)
length(brexit_polls$startdate)
library(tidyverse)
library(dslabs)
library(broom)
options(digits = )
options(digits = 3)
data("brexit_polls")
month(brexit_polls$startdate)
library(lubridate)
month(brexit_polls$startdate)
sum(month(brexit_polls$startdate) == 4)
?round_date
str(brexit_polls)
head(brexit_polls$enddate)
round_date(brexe)
round_date(brexit_polls$enddate, unit = "week")
sum(round_date(brexit_polls$enddate, unit = "week") == "2016-06-12")
weekdays(brexit_polls$enddate)
count(weekdays(brexit_polls$enddate))
brexit_polls <- brexit_polls %>% mutate(endday = weekdays(enddate))
brexit_polls
group_by(brexit_polls, endday)
group_by(brexit_polls, endday) %>% count(endday)
group_by(brexit_polls, endday) %>% count(endday) %>% max(n)
group_by(brexit_polls, endday) %>% count(endday)
data("movielens")
str(movielens)
movielens %>% tibble()
movielens %>% tibble() %>% count(genres)
movielens %>% tibble() %>% count(genres, movies)
?as_datetime
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01"))
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_year = year(timestamp)) %>% group_by(r_year) %>% count(r_year)
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_year = year(timestamp)) %>% group_by(r_year) %>% count(r_year) %>% .$n
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_year = year(timestamp)) %>% group_by(r_year) %>% count(r_year) %>% .$n %>% maz
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_year = year(timestamp)) %>% group_by(r_year) %>% count(r_year) %>% .$n %>% max()
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_hour = hour(timestamp)) %>% group_by(r_year) %>% count(r_year) %>% .$n %>% max()
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_hour = hour(timestamp)) %>% group_by(r_hour) %>% count(r_hour) %>% .$n %>% max()
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_hour = hour(timestamp)) %>% group_by(r_hour) %>% count(r_hour) %>% .$n
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_hour = hour(timestamp)) %>% group_by(r_hour) %>% count(r_hour)
movielens %>% tibble() %>% mutate(timestamp = as_datetime(timestamp, origin = "1970-01-01")) %>% mutate(r_hour = hour(timestamp)) %>% group_by(r_hour) %>% count(r_hour) %>% View()
install.packages("torch")
library(torch)
r --version
R --version
library(torch)
R.version
cuda_is_available()
cuda_current_device()
cuda_device_count()
array?
a
?array
?runif
x <- array(runif(8), dim = c(2,2,2))
y <- torch_tensor(x, dtype = torch_float64())
x
y
View(x)
View(y)
print(x)
?x
x + y
y + x
identical(x, as_array(y))
as.array(y)
identical(x, as.array(x))
identical(x, as.array(y))
x <- torch_tensor(1, requires_grad = T)
y <- torch_tensor(2, requires_grad = $)
y <- torch_tensor(2, requires_grad = T)
lis
ls
ls()
rm(y)
x
w <- torch_tensor(2, requires_grad = )
w <- torch_tensor(2, requires_grad = T)
b <- torch_tensor(3, requires_grad = T)
y <- w * x + b
y
y$backward()
y
x$grad
x
w$grad
b$grad
?backward
??backward
list.dirs()
rm(list=ls())
library(tidyverse)
library(reticulate)
install.packages("reticulate")
library(reticulate)
reticulate::conda_list()
reticulate::conda_list('py3.8')
reticulate::conda_list(py3.8)
use_condaenv("py3.8", required = T)
py_config()
{python}
install.packages("caret")
library(caret)
getwd()
setwd("Case_Studies/Movies/")
library(tidyverse)
files <- list.files()
files
files <- list.dirs()
files
files <- list.files("/data/")
files
files <- list.files("data/")
files
df = read_csv(files[1])
df = read_csv(getwd() + "data/" + files[1])
"a" + "b"
df = read_csv(paste(getwd(),"data/",files[1]))
df = read_csv(paste(getwd(),"data/",files[1], sep = ""))
df = read_csv(paste(getwd(),"/data/",files[1], sep = ""))
df.head()
summarise(df)
df
df.info
glimpse(df)
head(df)
top_n(df, 10)
